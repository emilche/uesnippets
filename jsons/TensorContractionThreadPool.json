[
  {
    "label": "TensorEvaluator",
    "kind": "Type",
    "detail": "struct declaration",
    "insertText": "TensorEvaluator"
  },
  {
    "label": "NoCallback",
    "kind": "Type",
    "detail": "struct declaration",
    "insertText": "NoCallback"
  },
  {
    "label": "EvalParallelNotification",
    "kind": "Type",
    "detail": "class declaration",
    "insertText": "EvalParallelNotification"
  },
  {
    "label": "EvalParallelContext",
    "kind": "Type",
    "detail": "class declaration",
    "insertText": "EvalParallelContext"
  },
  {
    "label": "ThreadLocalBlocks",
    "kind": "Type",
    "detail": "class declaration",
    "insertText": "ThreadLocalBlocks"
  },
  {
    "label": "ThreadLocalBlocksInitialize",
    "kind": "Type",
    "detail": "class declaration",
    "insertText": "ThreadLocalBlocksInitialize"
  },
  {
    "label": "ThreadLocalBlocksAllocator",
    "kind": "Type",
    "detail": "struct declaration",
    "insertText": "ThreadLocalBlocksAllocator"
  },
  {
    "label": "ThreadLocalBlocksRelease",
    "kind": "Type",
    "detail": "class declaration",
    "insertText": "ThreadLocalBlocksRelease"
  },
  {
    "label": "EvalShardedByInnerDimContext",
    "kind": "Type",
    "detail": "struct declaration",
    "insertText": "EvalShardedByInnerDimContext"
  },
  {
    "label": "int()",
    "kind": "Method",
    "detail": "Function (# ifndef EIGEN_CXX11_TENSOR_TENSOR_CONTRACTION_THREAD_POOL_H # define EIGEN_CXX11_TENSOR_TENSOR_CONTRACTION_THREAD_POOL_H # ifdef EIGEN_USE_THREADS namespace Eigen { template<Indices,LeftArgType,RightArgType,OutputKernelType> struct TensorEvaluator<const TensorContractionOp<Indices,LeftArgType,RightArgType,OutputKernelType>,ThreadPoolDevice> : public TensorContractionEvaluatorBase<TensorEvaluator<const TensorContractionOp<Indices,LeftArgType,RightArgType,OutputKernelType>,ThreadPoolDevice>> { typedef ThreadPoolDevice Device ; typedef TensorEvaluator<const TensorContractionOp<Indices,LeftArgType,RightArgType,OutputKernelType>,Device> Self ; typedef TensorContractionEvaluatorBase<Self> Base ; typedef TensorContractionOp<Indices,LeftArgType,RightArgType,OutputKernelType> XprType ; typedef internal::remove_const<XprType::Scalar>::type Scalar ; typedef XprType::Index Index ; typedef XprType::CoeffReturnType CoeffReturnType ; typedef PacketType<CoeffReturnType,Device>::type PacketReturnType ; enum { Layout = TensorEvaluator<LeftArgType,Device>::Layout,} ; typedef internal::conditional<static_cast<)",
    "insertText": "int(Layout) == static_cast<int>(ColMajor)"
  },
  {
    "label": "TensorEvaluator()",
    "kind": "Method",
    "detail": "Function (const int LDims = internal::array_size<TensorEvaluator<EvalLeftArgType,Device>::Dimensions>::value ; const int RDims = internal::array_size<TensorEvaluator<EvalRightArgType,Device>::Dimensions>::value ; const int ContractDims = internal::array_size<Indices>::value ; typedef array<Index,LDims> left_dim_mapper_t ; typedef array<Index,RDims> right_dim_mapper_t ; typedef array<Index,ContractDims> contract_t ; typedef array<Index,LDims - ContractDims> left_nocontract_t ; typedef array<Index,RDims - ContractDims> right_nocontract_t ; const int NumDims = LDims + RDims - 2* ContractDims ; typedef DSizes<Index,NumDims> Dimensions ; typedef internal::remove_const<EvalLeftArgType::Scalar>::type LhsScalar ; typedef internal::remove_const<EvalRightArgType::Scalar>::type RhsScalar ; typedef internal::gebp_traits<LhsScalar,RhsScalar> Traits ; typedef TensorEvaluator<EvalLeftArgType,Device> LeftEvaluator ; typedef TensorEvaluator<EvalRightArgType,Device> RightEvaluator ;)",
    "insertText": "TensorEvaluator(const XprType& op, const Device& device) : Base(op, device)"
  },
  {
    "label": "evalProduct()",
    "kind": "Method",
    "detail": "Function (} template<int Alignment> void)",
    "insertText": "evalProduct(Scalar* buffer)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (evalProductImpl<NoCallback,)",
    "insertText": "Alignment(buffer, NoCallback())"
  },
  {
    "label": "evalProductAsync()",
    "kind": "Method",
    "detail": "Function (} template<EvalToCallback,int Alignment> void)",
    "insertText": "evalProductAsync(Scalar* buffer, EvalToCallback done)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (evalProductImpl<EvalToCallback,)",
    "insertText": "Alignment(buffer, std::move(done))"
  },
  {
    "label": "evalProductImpl()",
    "kind": "Method",
    "detail": "Function (} template<DoneCallback,int Alignment> void)",
    "insertText": "evalProductImpl(Scalar* buffer, DoneCallback done)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (const bool IsEvalInSyncMode = std::is_same<DoneCallback,NoCallback>::value ; const Index m = this -> m_i_size ; const Index n = this -> m_j_size ; const Index k = this -> m_k_size ;)",
    "insertText": "if(m == 0 || n == 0 || k == 0)"
  },
  {
    "label": "shardByCol()",
    "kind": "Method",
    "detail": "Function (bool shard_by_col =)",
    "insertText": "shardByCol(m, n, 2)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (Index bm,bn,bk ;)",
    "insertText": "if(shard_by_col)"
  },
  {
    "label": "blocking()",
    "kind": "Method",
    "detail": "Function (internal::TensorContractionBlocking<Scalar,LhsScalar,RhsScalar,Index,internal::ShardByCol>)",
    "insertText": "blocking(k, m, n, 2)"
  },
  {
    "label": "mc()",
    "kind": "Method",
    "detail": "Function (bm = blocking .)",
    "insertText": "mc()"
  },
  {
    "label": "nc()",
    "kind": "Method",
    "detail": "Function (bn = blocking .)",
    "insertText": "nc()"
  },
  {
    "label": "kc()",
    "kind": "Method",
    "detail": "Function (bk = blocking .)",
    "insertText": "kc()"
  },
  {
    "label": "contractionCost()",
    "kind": "Method",
    "detail": "Function (} const TensorOpCost cost =)",
    "insertText": "contractionCost(m, n, bm, bn, bk, shard_by_col, false)"
  },
  {
    "label": "numThreads()",
    "kind": "Method",
    "detail": "Function (int num_threads = TensorCostModel<ThreadPoolDevice)",
    "insertText": "numThreads(static_cast<double>(n) * m, cost, this->m_device.numThreads())"
  },
  {
    "label": "numThreadsInnerDim()",
    "kind": "Method",
    "detail": "Function (int num_threads_by_k =)",
    "insertText": "numThreadsInnerDim(m, n, k)"
  },
  {
    "label": "ctx()",
    "kind": "Method",
    "detail": "Function (EvalShardedByInnerDimContext<DoneCallback>)",
    "insertText": "ctx(this, num_threads_by_k, buffer, m, n, k, std::move(done))"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (ctx . template run<)",
    "insertText": "Alignment()"
  },
  {
    "label": "DoneCallback()",
    "kind": "Method",
    "detail": "Function (} else { auto* ctx = new EvalShardedByInnerDimContext<)",
    "insertText": "DoneCallback(this, num_threads_by_k, buffer, m, n, k, std::move(done))"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (} return ; })",
    "insertText": "if(n == 1)"
  },
  {
    "label": "TENSOR_CONTRACTION_DISPATCH()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "TENSOR_CONTRACTION_DISPATCH(this->template evalProductSequential, Unaligned, (buffer))"
  },
  {
    "label": "blocking()",
    "kind": "Method",
    "detail": "Function (internal::TensorContractionBlocking<Scalar,LhsScalar,RhsScalar,Index,internal::ShardByCol>)",
    "insertText": "blocking(k, m, n, num_threads)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (} Index nm0 =)",
    "insertText": "divup(m, bm)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (Index nn0 =)",
    "insertText": "divup(n, bn)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (Index nk =)",
    "insertText": "divup(k, bk)"
  },
  {
    "label": "coarsenM()",
    "kind": "Method",
    "detail": "Function (gm =)",
    "insertText": "coarsenM(m, n, bm, bn, bk, gn, num_threads, shard_by_col)"
  },
  {
    "label": "coarsenN()",
    "kind": "Method",
    "detail": "Function (gn =)",
    "insertText": "coarsenN(m, n, bm, bn, bk, gm, num_threads, shard_by_col)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (} Index nm =)",
    "insertText": "divup(nm0, gm)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (Index nn =)",
    "insertText": "divup(nn0, gn)"
  },
  {
    "label": "numThreadsInPool()",
    "kind": "Method",
    "detail": "Function (const Index sharding_dim_tasks = shard_by_col ? nn : nm ; const int num_worker_threads = this -> m_device .)",
    "insertText": "numThreadsInPool()"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (const float oversharding_factor = num_worker_threads<= 4 ? 8 . 0 : num_worker_threads<= 8 ? 4 . 0 : num_worker_threads<= 1 6 ? 2 . 0 : num_worker_threads<= 3 2 ? 1 . 0 : num_worker_threads<= 6 4 ? 0 . 8 : 0 . 6 ; const bool parallelize_by_sharding_dim_only = sharding_dim_tasks> = oversharding_factor* num_worker_threads ; bool parallel_pack = num_threads> = nm* nn ;)",
    "insertText": "if(m * bk * Index(sizeof(LhsScalar)) + n * bk * Index(sizeof(RhsScalar)) <= l2CacheSize() * num_threads)"
  },
  {
    "label": "CONTEXT_ARGS()",
    "kind": "Method",
    "detail": "Function (# define)",
    "insertText": "CONTEXT_ARGS(this, num_threads, buffer, m, n, k, bm, bn, bk, nm, nn, nk, gm, gn, nm0, \\ nn0, shard_by_col, parallel_pack, parallelize_by_sharding_dim_only, \\ NoCallback()) \\ .run() TENSOR_CONTRACTION_DISPATCH(SyncEvalParallelContext, Alignment, CONTEXT_ARGS)"
  },
  {
    "label": "CONTEXT_ARGS()",
    "kind": "Method",
    "detail": "Function (# undef CONTEXT_ARGS } else { # define)",
    "insertText": "CONTEXT_ARGS(this, num_threads, buffer, m, n, k, bm, bn, bk, nm, nn, nk, gm, gn, nm0, \\ nn0, shard_by_col, parallel_pack, parallelize_by_sharding_dim_only, \\ std::move(done)) TENSOR_CONTRACTION_ASYNC_DISPATCH(EvalParallelContext, DoneCallback, Alignment, CONTEXT_ARGS, run())"
  },
  {
    "label": "operator()",
    "kind": "Method",
    "detail": "Function (# undef CONTEXT_ARGS } } struct NoCallback { void)",
    "insertText": "operator()()"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(false && \"NoCallback should never be called\")"
  },
  {
    "label": "EvalParallelNotification()",
    "kind": "Method",
    "detail": "Function (} } ; template<DoneCallback,Context> class EvalParallelNotification ; template<Context> class EvalParallelNotification<NoCallback,Context> { public :)",
    "insertText": "EvalParallelNotification(Context*, NoCallback)"
  },
  {
    "label": "Notify()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "Notify()"
  },
  {
    "label": "Wait()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "Wait()"
  },
  {
    "label": "EvalParallelNotification()",
    "kind": "Method",
    "detail": "Function (} private : Eigen::Notification done_ ; } ; template<DoneCallback,Context> class EvalParallelNotification { public :)",
    "insertText": "EvalParallelNotification(Context* ctx, DoneCallback done) : ctx_(ctx), done_(std::move(done))"
  },
  {
    "label": "done_copy()",
    "kind": "Method",
    "detail": "Function (delete ctx_ ;)",
    "insertText": "done_copy()"
  },
  {
    "label": "EvalParallelContext()",
    "kind": "Method",
    "detail": "Function (} private : Context* ctx_ ; DoneCallback done_ ; } ; template<DoneCallback,bool lhs_inner_dim_contiguous,bool rhs_inner_dim_contiguous,bool rhs_inner_dim_reordered,int Alignment> class EvalParallelContext { public : typedef internal::TensorContractionInputMapper<LhsScalar,Index,internal::Lhs,LeftEvaluator,left_nocontract_t,contract_t,internal::packet_traits<LhsScalar>::size,lhs_inner_dim_contiguous,false,Unaligned> LhsMapper ; typedef internal::TensorContractionInputMapper<RhsScalar,Index,internal::Rhs,RightEvaluator,right_nocontract_t,contract_t,internal::packet_traits<RhsScalar>::size,rhs_inner_dim_contiguous,rhs_inner_dim_reordered,Unaligned> RhsMapper ; typedef internal::blas_data_mapper<Scalar,Index,ColMajor> OutputMapper ; typedef internal::TensorContractionKernel<Scalar,LhsScalar,RhsScalar,Index,OutputMapper,LhsMapper,RhsMapper> TensorContractionKernel ; typedef TensorContractionKernel::LhsBlock LhsBlock ; typedef TensorContractionKernel::RhsBlock RhsBlock ; typedef TensorContractionKernel::BlockMemHandle BlockMemHandle ;)",
    "insertText": "EvalParallelContext(const Self* self, int num_threads, Scalar* buffer, Index tm, Index tn, Index tk, Index bm, Index bn, Index bk, Index nm, Index nn, Index nk, Index gm, Index gn, Index nm0, Index nn0, bool shard_by_col, bool parallel_pack, bool parallelize_by_sharding_dim_only, DoneCallback done) : created_by_thread_id_(std::this_thread::get_id()), done_(this, std::move(done)), device_(self->m_device), lhs_(self->m_leftImpl, self->m_left_nocontract_strides, self->m_i_strides, self->m_left_contracting_strides, self->m_k_strides), rhs_(self->m_rightImpl, self->m_right_nocontract_strides, self->m_j_strides, self->m_right_contracting_strides, self->m_k_strides), buffer_(buffer), output_(buffer, tm), output_kernel_(self->m_output_kernel), tensor_contraction_params_(self->m_tensor_contraction_params), num_threads_(num_threads), shard_by_col_(shard_by_col), parallel_pack_(parallel_pack), parallelize_by_sharding_dim_only_(parallelize_by_sharding_dim_only), m_(tm), n_(tn), k_(tk), bm_(bm), bn_(bn), bk_(bk), nm_(nm), nn_(nn), nk_(nk), gm_(gm), gn_(gn), nm0_(nm0), nn0_(nn0), kernel_(m_, k_, n_, bm_, bk_, bn_), num_thread_local_allocations_(0), thread_local_capacity(2 * (parallelize_by_sharding_dim_only_ ? device_.numThreadsInPool() : 0)), lhs_thread_local_blocks_(shard_by_col_ ? 0 : thread_local_capacity, {*this}, {*this}), rhs_thread_local_blocks_(shard_by_col_ ? thread_local_capacity : 0, {*this}, {*this})"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(!(parallel_pack && parallelize_by_sharding_dim_only))"
  },
  {
    "label": "x()",
    "kind": "Method",
    "detail": "Function (state_switch_ [ x ] =)",
    "insertText": "x(parallel_pack_ ? nn_ + nm_ : (shard_by_col_ ? nn_ : nm_)) + (x == P - 1 ? nm_ * nn_ : 0)"
  },
  {
    "label": "parallel_pack_()",
    "kind": "Method",
    "detail": "Function (state_packing_ready_ [ x ] =)",
    "insertText": "parallel_pack_(shard_by_col_ ? nm_ : nn_)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (state_kernel_ [ x ] = new std::atomic<uint8_t>* [ nm_ ] ;)",
    "insertText": "for(Index m = 0; m < nm_; m++)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (state_kernel_ [ x ] [ m ] = new std::atomic<uint8_t> [ nn_ ] ;)",
    "insertText": "for(Index n = 0; n < nn_; n++) state_kernel_[x][m][n].store( (x == 0 ? 0 : 1) + (parallel_pack_ ? 2 : 1), std::memory_order_relaxed)"
  },
  {
    "label": "allocateSlices()",
    "kind": "Method",
    "detail": "Function (} } packed_mem_ = kernel_ .)",
    "insertText": "allocateSlices(device_, nm0_, nn0_, std::min<Index>(nk_, P - 1), packed_lhs_, packed_rhs_)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (can_use_thread_local_packed_ = new std::atomic<bool> [ nn_ ] ;)",
    "insertText": "for(int i = 0; i < nn_; ++i) can_use_thread_local_packed_[i].store(true, std::memory_order_relaxed)"
  },
  {
    "label": "allocateSlices()",
    "kind": "Method",
    "detail": "Function (Index num_blocks = num_worker_threads* gn_ ; thread_local_pre_alocated_mem_ = kernel_ .)",
    "insertText": "allocateSlices(device_, 0, num_blocks, 1, nullptr, &rhs_thread_local_pre_allocated_)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (} else { can_use_thread_local_packed_ = new std::atomic<bool> [ nm_ ] ;)",
    "insertText": "for(int i = 0; i < nm_; ++i) can_use_thread_local_packed_[i].store(true, std::memory_order_relaxed)"
  },
  {
    "label": "allocateSlices()",
    "kind": "Method",
    "detail": "Function (Index num_blocks = num_worker_threads* gm_ ; thread_local_pre_alocated_mem_ = kernel_ .)",
    "insertText": "allocateSlices(device_, num_blocks, 0, 1, &lhs_thread_local_pre_allocated_, nullptr)"
  },
  {
    "label": "EvalParallelContext()",
    "kind": "Method",
    "detail": "Function (} } } ~)",
    "insertText": "EvalParallelContext()"
  },
  {
    "label": "deallocate()",
    "kind": "Method",
    "detail": "Function (delete [ ] state_kernel_ [ x ] ; } kernel_ .)",
    "insertText": "deallocate(device_, packed_mem_)"
  },
  {
    "label": "deallocate()",
    "kind": "Method",
    "detail": "Function (kernel_ .)",
    "insertText": "deallocate(device_, thread_local_pre_alocated_mem_)"
  },
  {
    "label": "run()",
    "kind": "Method",
    "detail": "Function (delete [ ] can_use_thread_local_packed_ ; } } void)",
    "insertText": "run()"
  },
  {
    "label": "signal_switch()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "signal_switch(0, 1)"
  },
  {
    "label": "ThreadLocalBlocks()",
    "kind": "Method",
    "detail": "Function (} private : std::thread::id created_by_thread_id_ ; EvalParallelNotification<DoneCallback,EvalParallelContext> done_ ; const Device& device_ ; LhsMapper lhs_ ; RhsMapper rhs_ ; Scalar* const buffer_ ; OutputMapper output_ ; OutputKernelType output_kernel_ ; TensorContractionParams tensor_contraction_params_ ; const int num_threads_ ; const bool shard_by_col_ ; const bool parallel_pack_ ; const bool parallelize_by_sharding_dim_only_ ; const Index m_ ; const Index n_ ; const Index k_ ; const Index bm_ ; const Index bn_ ; const Index bk_ ; const Index nm_ ; const Index nn_ ; const Index nk_ ; const Index gm_ ; const Index gn_ ; const Index nm0_ ; const Index nn0_ ; TensorContractionKernel kernel_ ; const Index P = 3 ; BlockMemHandle packed_mem_ ; std::vector<LhsBlock> packed_lhs_ [ P - 1 ] ; std::vector<RhsBlock> packed_rhs_ [ P - 1 ] ; BlockMemHandle thread_local_pre_alocated_mem_ ; std::vector<LhsBlock> lhs_thread_local_pre_allocated_ ; std::vector<RhsBlock> rhs_thread_local_pre_allocated_ ; std::atomic<int> num_thread_local_allocations_ ; const int thread_local_capacity ; template<BlockType> class ThreadLocalBlocks { public :)",
    "insertText": "ThreadLocalBlocks()"
  },
  {
    "label": "ThreadLocalBlocks()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "ThreadLocalBlocks(BlockType* base, size_t grain_size) : is_pre_allocated_(true), thread_local_pre_allocated_base_(base), grain_size_(grain_size)"
  },
  {
    "label": "ThreadLocalBlocks()",
    "kind": "Method",
    "detail": "Function (})",
    "insertText": "ThreadLocalBlocks(BlockMemHandle mem_handle, std::vector<BlockType> blocks) : is_pre_allocated_(false), mem_handle_(std::move(mem_handle)), blocks_(std::move(blocks))"
  },
  {
    "label": "block()",
    "kind": "Method",
    "detail": "Function (} BlockType&)",
    "insertText": "block(int grain_index)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(grain_index >= 0)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(static_cast<size_t>(grain_index) < size())"
  },
  {
    "label": "deallocate()",
    "kind": "Method",
    "detail": "Function (ctx . kernel_ .)",
    "insertText": "deallocate(ctx.device_, mem_handle_)"
  },
  {
    "label": "size()",
    "kind": "Method",
    "detail": "Function (} } size_t)",
    "insertText": "size()"
  },
  {
    "label": "static_assert()",
    "kind": "Method",
    "detail": "Function (} private : bool is_pre_allocated_ ; BlockType* thread_local_pre_allocated_base_ = nullptr ; size_t grain_size_ = 0 ; BlockMemHandle mem_handle_ { } ; std::vector<BlockType> blocks_ ; } ; template<BlockType,bool is_rhs> class ThreadLocalBlocksInitialize { bool kIsLhs = ! is_rhs&& std::is_same<BlockType,LhsBlock>::value ; const bool kIsRhs = is_rhs&& std::is_same<BlockType,RhsBlock>::value ;)",
    "insertText": "static_assert(kIsLhs || kIsRhs, \"Unkown block type\")"
  },
  {
    "label": "ThreadLocalBlocksInitialize()",
    "kind": "Method",
    "detail": "Function (using Blocks = ThreadLocalBlocks<BlockType> ; public :)",
    "insertText": "ThreadLocalBlocksInitialize(EvalParallelContext& ctx) : ctx_(ctx), num_worker_threads_(ctx_.device_.numThreadsInPool())"
  },
  {
    "label": "operator()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "operator()(Blocks& blocks)"
  },
  {
    "label": "fetch_add()",
    "kind": "Method",
    "detail": "Function (const int n = ctx_ . num_thread_local_allocations_ .)",
    "insertText": "fetch_add(1, std::memory_order_relaxed)"
  },
  {
    "label": "allocate()",
    "kind": "Method",
    "detail": "Function (ThreadLocalBlocksAllocator<is_rhs)",
    "insertText": "allocate(ctx_, blocks)"
  },
  {
    "label": "reuse()",
    "kind": "Method",
    "detail": "Function (} else { ThreadLocalBlocksAllocator<is_rhs)",
    "insertText": "reuse(ctx_, n, blocks)"
  },
  {
    "label": "allocate()",
    "kind": "Method",
    "detail": "Function (} } private : template<bool pack_rhs,EvalCtx = EvalParallelContext> struct ThreadLocalBlocksAllocator ; template<EvalCtx> struct ThreadLocalBlocksAllocator<true,EvalCtx> { void)",
    "insertText": "allocate(EvalCtx& ctx, Blocks& blocks)"
  },
  {
    "label": "allocateSlices()",
    "kind": "Method",
    "detail": "Function (std::vector<RhsBlock> rhs_blocks ; BlockMemHandle mem_handle = ctx . kernel_ .)",
    "insertText": "allocateSlices(ctx.device_, 0, ctx.gn_, 1, nullptr, &rhs_blocks)"
  },
  {
    "label": "RhsBlock()",
    "kind": "Method",
    "detail": "Function (blocks = ThreadLocalBlocks<)",
    "insertText": "RhsBlock(std::move(mem_handle), std::move(rhs_blocks))"
  },
  {
    "label": "reuse()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "reuse(EvalCtx& ctx, int index, Blocks& blocks)"
  },
  {
    "label": "RhsBlock()",
    "kind": "Method",
    "detail": "Function (RhsBlock* ptr =& ctx . rhs_thread_local_pre_allocated_ [ ctx . gn_* index ] ; blocks = ThreadLocalBlocks<)",
    "insertText": "RhsBlock(ptr, ctx.gn_)"
  },
  {
    "label": "allocateSlices()",
    "kind": "Method",
    "detail": "Function (std::vector<LhsBlock> lhs_blocks ; BlockMemHandle mem_handle = ctx . kernel_ .)",
    "insertText": "allocateSlices(ctx.device_, ctx.gm_, 0, 1, &lhs_blocks, nullptr)"
  },
  {
    "label": "LhsBlock()",
    "kind": "Method",
    "detail": "Function (blocks = ThreadLocalBlocks<)",
    "insertText": "LhsBlock(std::move(mem_handle), std::move(lhs_blocks))"
  },
  {
    "label": "LhsBlock()",
    "kind": "Method",
    "detail": "Function (LhsBlock* ptr =& ctx . lhs_thread_local_pre_allocated_ [ ctx . gm_* index ] ; blocks = ThreadLocalBlocks<)",
    "insertText": "LhsBlock(ptr, ctx.gm_)"
  },
  {
    "label": "ThreadLocalBlocksRelease()",
    "kind": "Method",
    "detail": "Function (} } ; EvalParallelContext& ctx_ ; const int num_worker_threads_ ; } ; template<BlockType> class ThreadLocalBlocksRelease { public : using Blocks = ThreadLocalBlocks<BlockType> ;)",
    "insertText": "ThreadLocalBlocksRelease(EvalParallelContext& ctx) : ctx_(ctx)"
  },
  {
    "label": "Release()",
    "kind": "Method",
    "detail": "Function (blocks .)",
    "insertText": "Release(ctx_)"
  },
  {
    "label": "packed_lhs()",
    "kind": "Method",
    "detail": "Function (} private : EvalParallelContext& ctx_ ; } ; using ThreadLocalLhsInit = ThreadLocalBlocksInitialize<LhsBlock,false> ; using ThreadLocalRhsInit = ThreadLocalBlocksInitialize<RhsBlock,true> ; using ThreadLocalLhsRelease = ThreadLocalBlocksRelease<LhsBlock> ; using ThreadLocalRhsRelease = ThreadLocalBlocksRelease<RhsBlock> ; Eigen::ThreadLocal<ThreadLocalBlocks<LhsBlock>,ThreadLocalLhsInit,ThreadLocalLhsRelease> lhs_thread_local_blocks_ ; Eigen::ThreadLocal<ThreadLocalBlocks<RhsBlock>,ThreadLocalRhsInit,ThreadLocalRhsRelease> rhs_thread_local_blocks_ ; std::atomic<bool>* can_use_thread_local_packed_ ; std::atomic<uint8_t>** state_kernel_ [ P ] ; char pad_ [ 1 2 8 ] ; std::atomic<Index> state_packing_ready_ [ P ] ; std::atomic<Index> state_switch_ [ P ] ; LhsBlock&)",
    "insertText": "packed_lhs(Index m, Index k, Index m1, bool use_thread_local)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(!shard_by_col_)"
  },
  {
    "label": "local()",
    "kind": "Method",
    "detail": "Function (ThreadLocalBlocks<LhsBlock>& blocks = lhs_thread_local_blocks_ .)",
    "insertText": "local()"
  },
  {
    "label": "block()",
    "kind": "Method",
    "detail": "Function (Index grain_index = m1 - m* gm_ ; return blocks .)",
    "insertText": "block(internal::convert_index<int>(grain_index))"
  },
  {
    "label": "k()",
    "kind": "Method",
    "detail": "Function (} else { return packed_lhs_ [)",
    "insertText": "k(P - 1)"
  },
  {
    "label": "packed_rhs()",
    "kind": "Method",
    "detail": "Function (} } RhsBlock&)",
    "insertText": "packed_rhs(Index n, Index k, Index n1, bool use_thread_local)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(shard_by_col_)"
  },
  {
    "label": "pack_lhs()",
    "kind": "Method",
    "detail": "Function (} } void)",
    "insertText": "pack_lhs(Index m, Index k)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (bool use_thread_local = false ;)",
    "insertText": "if(parallelize_by_sharding_dim_only_ && !shard_by_col_ && can_use_thread_local_packed_[m].load(std::memory_order_relaxed))"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (use_thread_local = true ; } else {)",
    "insertText": "eigen_assert(k > 0)"
  },
  {
    "label": "store()",
    "kind": "Method",
    "detail": "Function (can_use_thread_local_packed_ [ m ] .)",
    "insertText": "store(false, std::memory_order_relaxed)"
  },
  {
    "label": "gm()",
    "kind": "Method",
    "detail": "Function (} } const Index mend = m* gm_ +)",
    "insertText": "gm(m)"
  },
  {
    "label": "assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "assert(!use_thread_local)"
  },
  {
    "label": "signal_packing()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "signal_packing(k)"
  },
  {
    "label": "signal_switch()",
    "kind": "Method",
    "detail": "Function (} else {)",
    "insertText": "signal_switch(k + 1)"
  },
  {
    "label": "signal_kernel()",
    "kind": "Method",
    "detail": "Function (bool sync = parallelize_by_sharding_dim_only_ | | n = = 0 ;)",
    "insertText": "signal_kernel(m, n, k, sync, use_thread_local)"
  },
  {
    "label": "pack_rhs()",
    "kind": "Method",
    "detail": "Function (} } } void)",
    "insertText": "pack_rhs(Index n, Index k)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (bool use_thread_local = false ;)",
    "insertText": "if(parallelize_by_sharding_dim_only_ && shard_by_col_ && can_use_thread_local_packed_[n].load(std::memory_order_relaxed))"
  },
  {
    "label": "gn()",
    "kind": "Method",
    "detail": "Function (} } const Index nend = n* gn_ +)",
    "insertText": "gn(n)"
  },
  {
    "label": "memset()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "memset(buffer_ + n1 * bn_ * m_, 0, bn(n1) * m_ * sizeof(Scalar))"
  },
  {
    "label": "packRhs()",
    "kind": "Method",
    "detail": "Function (} kernel_ .)",
    "insertText": "packRhs(&packed_rhs(n, k, n1, use_thread_local), rhs_.getSubMapper(k * bk_, n1 * bn_), bk(k), bn(n1))"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (})",
    "insertText": "if(parallel_pack_ || shard_by_col_)"
  },
  {
    "label": "kernel()",
    "kind": "Method",
    "detail": "Function (} } void)",
    "insertText": "kernel(Index m, Index n, Index k, bool use_thread_local)"
  },
  {
    "label": "Scalar()",
    "kind": "Method",
    "detail": "Function (const Scalar alpha =)",
    "insertText": "Scalar(1)"
  },
  {
    "label": "beta()",
    "kind": "Method",
    "detail": "Function (const Scalar)",
    "insertText": "beta(TensorContractionKernel::HasBeta && k == 0) ? Scalar(0) : Scalar(1)"
  },
  {
    "label": "getSubMapper()",
    "kind": "Method",
    "detail": "Function (const auto output_mapper = output_ .)",
    "insertText": "getSubMapper(m1 * bm_, n1 * bn_)"
  },
  {
    "label": "invoke()",
    "kind": "Method",
    "detail": "Function (kernel_ .)",
    "insertText": "invoke(output_mapper, packed_lhs(m, k, m1, !shard_by_col_ && use_thread_local), packed_rhs(n, k, n1, shard_by_col_ && use_thread_local), bm(m1), bk(k), bn(n1), alpha, beta)"
  },
  {
    "label": "output_kernel_()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "output_kernel_(output_mapper, tensor_contraction_params_, m1 * bm_, n1 * bn_, bm(m1), bn(n1))"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (} } } } else {)",
    "insertText": "for(Index m1 = m * gm_; m1 < mend; m1++) for (Index n1 = n * gn_; n1 < nend; n1++)"
  },
  {
    "label": "signal_kernel()",
    "kind": "Method",
    "detail": "Function (} } })",
    "insertText": "signal_kernel(m, n, k + 1, false, false)"
  },
  {
    "label": "signal_switch()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "signal_switch(k + 2)"
  },
  {
    "label": "signal_packing()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "signal_packing(Index k)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(!parallel_pack_)"
  },
  {
    "label": "fetch_sub()",
    "kind": "Method",
    "detail": "Function (Index s = state_packing_ready_ [ k % P ] .)",
    "insertText": "fetch_sub(1)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(s > 0)"
  },
  {
    "label": "enqueue_packing()",
    "kind": "Method",
    "detail": "Function (state_packing_ready_ [ k % P ] = shard_by_col_ ? nm_ : nn_ ;)",
    "insertText": "enqueue_packing(k, shard_by_col_)"
  },
  {
    "label": "signal_kernel()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "signal_kernel(Index m, Index n, Index k, bool sync, bool use_thread_local)"
  },
  {
    "label": "load()",
    "kind": "Method",
    "detail": "Function (std::atomic<uint8_t>* state =& state_kernel_ [ k % P ] [ m ] [ n ] ; Index s = state ->)",
    "insertText": "load()"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(!use_thread_local)"
  },
  {
    "label": "kernel()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "kernel(m, n, k, use_thread_local)"
  },
  {
    "label": "enqueueNoNotification()",
    "kind": "Method",
    "detail": "Function (device_ .)",
    "insertText": "enqueueNoNotification([=]() { kernel(m, n, k, use_thread_local); })"
  },
  {
    "label": "signal_switch()",
    "kind": "Method",
    "detail": "Function (} } void)",
    "insertText": "signal_switch(Index k, Index v = 1)"
  },
  {
    "label": "fetch_sub()",
    "kind": "Method",
    "detail": "Function (Index s = state_switch_ [ k % P ] .)",
    "insertText": "fetch_sub(v)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(s >= v)"
  },
  {
    "label": "P()",
    "kind": "Method",
    "detail": "Function (state_switch_ [ k %)",
    "insertText": "P(parallel_pack_ ? nm_ + nn_ : (shard_by_col_ ? nn_ : nm_))"
  },
  {
    "label": "enqueue_packing()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "enqueue_packing(k, !shard_by_col_)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (} else)",
    "insertText": "if(shard_by_col_)"
  },
  {
    "label": "enqueue_packing()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "enqueue_packing(k, false)"
  },
  {
    "label": "enqueue_packing()",
    "kind": "Method",
    "detail": "Function (} else {)",
    "insertText": "enqueue_packing(k, true)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (} } else)",
    "insertText": "if(k == nk_)"
  },
  {
    "label": "signal_switch()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "signal_switch(k + 1, parallel_pack_ ? nm_ + nn_ : (shard_by_col_ ? nn_ : nm_))"
  },
  {
    "label": "enqueue_packing()",
    "kind": "Method",
    "detail": "Function (} } void)",
    "insertText": "enqueue_packing(Index k, bool rhs)"
  },
  {
    "label": "enqueue_packing_helper()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "enqueue_packing_helper(0, rhs ? nn_ : nm_, k, rhs)"
  },
  {
    "label": "enqueue_packing_helper()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "enqueue_packing_helper(Index start, Index end, Index k, bool rhs)"
  },
  {
    "label": "while()",
    "kind": "Method",
    "detail": "Function (} else {)",
    "insertText": "while(end - start > 1)"
  },
  {
    "label": "mid()",
    "kind": "Method",
    "detail": "Function (Index)",
    "insertText": "mid(start + end)"
  },
  {
    "label": "enqueueNoNotification()",
    "kind": "Method",
    "detail": "Function (device_ .)",
    "insertText": "enqueueNoNotification([=]() { enqueue_packing_helper(mid, end, k, rhs); })"
  },
  {
    "label": "pack_async()",
    "kind": "Method",
    "detail": "Function (end = mid ; } bool)",
    "insertText": "pack_async(start == 0) && (parallelize_by_sharding_dim_only_&& shard_by_col_ == rhs) && (k > 0 || std::this_thread::get_id() == created_by_thread_id_)"
  },
  {
    "label": "enqueueNoNotification()",
    "kind": "Method",
    "detail": "Function (device_ .)",
    "insertText": "enqueueNoNotification([=]() { enqueue_packing_helper(start, end, k, rhs); })"
  },
  {
    "label": "enqueue_packing_helper()",
    "kind": "Method",
    "detail": "Function (} else {)",
    "insertText": "enqueue_packing_helper(start, end, k, rhs)"
  },
  {
    "label": "bm()",
    "kind": "Method",
    "detail": "Function (} } } Index)",
    "insertText": "bm(Index m)"
  },
  {
    "label": "operator()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "operator(const EvalParallelContext&)"
  },
  {
    "label": "EvalShardedByInnerDimContext()",
    "kind": "Method",
    "detail": "Function (} ; template<bool lhs_inner_dim_contiguous,bool rhs_inner_dim_contiguous,bool rhs_inner_dim_reordered,int Alignment> using SyncEvalParallelContext = EvalParallelContext<NoCallback,lhs_inner_dim_contiguous,rhs_inner_dim_contiguous,rhs_inner_dim_reordered,Alignment> ; template<DoneCallback> struct EvalShardedByInnerDimContext {)",
    "insertText": "EvalShardedByInnerDimContext(const Self* self, int num_threads, Scalar* result_buffer, Index m_size, Index n_size, Index k_size, DoneCallback done_callback) : evaluator(self), m_lhs_inner_dim_contiguous(evaluator->m_lhs_inner_dim_contiguous), m_rhs_inner_dim_contiguous(evaluator->m_rhs_inner_dim_contiguous), m_rhs_inner_dim_reordered(evaluator->m_rhs_inner_dim_reordered), result(result_buffer), m(m_size), n(n_size), k(k_size), done(std::move(done_callback)), buffer_size_bytes(m * n * sizeof(Scalar)), block_size(blockSize(k, num_threads)), num_blocks(divup<Index>(k, block_size)), num_pending_blocks(internal::convert_index<int>(num_blocks)), l0_ranges(divup<Index>(num_blocks, l0_size)), l0_state(l0_ranges), block_buffers(num_blocks)"
  },
  {
    "label": "actualRangeSize()",
    "kind": "Method",
    "detail": "Function (const Index num_pending_tasks =)",
    "insertText": "actualRangeSize(l0_ranges, l0_size, i)"
  },
  {
    "label": "emplace_back()",
    "kind": "Method",
    "detail": "Function (l0_state .)",
    "insertText": "emplace_back(internal::convert_index<int>(num_pending_tasks))"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (})",
    "insertText": "for(Index block_idx = 0; block_idx < num_blocks; ++block_idx)"
  },
  {
    "label": "Scalar()",
    "kind": "Method",
    "detail": "Function (Scalar* buf = block_idx = = 0 ? result : static_cast<)",
    "insertText": "Scalar(evaluator->m_device.allocate( buffer_size_bytes))"
  },
  {
    "label": "emplace_back()",
    "kind": "Method",
    "detail": "Function (block_buffers .)",
    "insertText": "emplace_back(buf)"
  },
  {
    "label": "EvalShardedByInnerDimContext()",
    "kind": "Method",
    "detail": "Function (} } ~)",
    "insertText": "EvalShardedByInnerDimContext()"
  },
  {
    "label": "deallocate()",
    "kind": "Method",
    "detail": "Function (evaluator -> m_device .)",
    "insertText": "deallocate(block_buffers[i])"
  },
  {
    "label": "barrier()",
    "kind": "Method",
    "detail": "Function (Barrier)",
    "insertText": "barrier(internal::convert_index<int>(num_blocks))"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (eval<)",
    "insertText": "Alignment(barrier, 0, num_blocks)"
  },
  {
    "label": "applyOutputKernel()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "applyOutputKernel()"
  },
  {
    "label": "runAsync()",
    "kind": "Method",
    "detail": "Function (} template<int Alignment> void)",
    "insertText": "runAsync()"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (evalAsync<)",
    "insertText": "Alignment(0, num_blocks)"
  },
  {
    "label": "processBlock()",
    "kind": "Method",
    "detail": "Function (} private : const Index packet_size = internal::packet_traits<RhsScalar>::size ; const Self* evaluator ; bool m_lhs_inner_dim_contiguous ; bool m_rhs_inner_dim_contiguous ; bool m_rhs_inner_dim_reordered ; Scalar* result ; Index m ; Index n ; Index k ; DoneCallback done ; Index buffer_size_bytes ; Index block_size ; Index num_blocks ; std::atomic<int> num_pending_blocks ; const Index l0_size = 4 ; Index l0_ranges ; MaxSizeVector<std::atomic<int>> l0_state ; MaxSizeVector<Scalar*> block_buffers ; template<int Alignment> void)",
    "insertText": "processBlock(Index block_idx, Index begin, Index end)"
  },
  {
    "label": "TENSOR_CONTRACTION_DISPATCH()",
    "kind": "Method",
    "detail": "Function (Scalar* buf = block_buffers [ block_idx ] ;)",
    "insertText": "TENSOR_CONTRACTION_DISPATCH(evaluator->template evalGemmPartialWithoutOutputKernel, Alignment, (buf, begin, end, internal::convert_index<int>(num_blocks)))"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(v >= 1)"
  },
  {
    "label": "actualRangeSize()",
    "kind": "Method",
    "detail": "Function (const Index rng_size =)",
    "insertText": "actualRangeSize(l0_ranges, l0_size, l0_index)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (const Index dst_block_idx = l0_index* l0_size ;)",
    "insertText": "if(rng_size == l0_size)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (addAllToBuffer<)",
    "insertText": "Alignment(m * n, block_buffers[dst_block_idx + 1], block_buffers[dst_block_idx + 2], block_buffers[dst_block_idx + 3], block_buffers[dst_block_idx])"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (} else {)",
    "insertText": "for(int i = 1; i < rng_size; ++i)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (addToBuffer<)",
    "insertText": "Alignment(m * n, block_buffers[dst_block_idx + i], block_buffers[dst_block_idx])"
  },
  {
    "label": "aggregateL0Blocks()",
    "kind": "Method",
    "detail": "Function (} } } } template<int Alignment> void)",
    "insertText": "aggregateL0Blocks()"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (Index l0_index = 1 ;)",
    "insertText": "for(; l0_index + 2 < l0_ranges; l0_index += 3)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (addAllToBuffer<)",
    "insertText": "Alignment(m * n, block_buffers[(l0_index + 0) * l0_size], block_buffers[(l0_index + 1) * l0_size], block_buffers[(l0_index + 2) * l0_size], block_buffers[0])"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (})",
    "insertText": "for(; l0_index < l0_ranges; ++l0_index)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (addToBuffer<)",
    "insertText": "Alignment(m * n, block_buffers[l0_index * l0_size], block_buffers[0])"
  },
  {
    "label": "m_output_kernel()",
    "kind": "Method",
    "detail": "Function (typedef internal::blas_data_mapper<Scalar,Index,ColMajor> OutputMapper ; evaluator ->)",
    "insertText": "m_output_kernel(OutputMapper(result, m), evaluator->m_tensor_contraction_params, static_cast<Eigen::Index>(0), static_cast<Eigen::Index>(0), m, n)"
  },
  {
    "label": "actualBlockSize()",
    "kind": "Method",
    "detail": "Function (} Index)",
    "insertText": "actualBlockSize(Index block_idx)"
  },
  {
    "label": "eigen_assert()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "eigen_assert(range_idx < num_ranges)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (const int output_packet_size = internal::unpacket_traits<PacketReturnType>::size ; size_t i = 0 ; const size_t num_packets = n / output_packet_size ;)",
    "insertText": "for(; i < output_packet_size * num_packets; i += output_packet_size)"
  },
  {
    "label": "PacketReturnType()",
    "kind": "Method",
    "detail": "Function (const PacketReturnType src_val = internal::pload<)",
    "insertText": "PacketReturnType(src_buf + i)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (const PacketReturnType tgt_val = internal::ploadt<PacketReturnType,)",
    "insertText": "Alignment(tgt_buf + i)"
  },
  {
    "label": "padd()",
    "kind": "Method",
    "detail": "Function (const PacketReturnType sum =)",
    "insertText": "padd(src_val, tgt_val)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (internal::pstoret<Scalar,PacketReturnType,)",
    "insertText": "Alignment(tgt_buf + i, sum)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (})",
    "insertText": "for(; i < n; ++i)"
  },
  {
    "label": "addAllToBuffer()",
    "kind": "Method",
    "detail": "Function (tgt_buf [ i ] + = src_buf [ i ] ; } } template<int Alignment> EIGEN_STRONG_INLINE void)",
    "insertText": "addAllToBuffer(size_t n, const Scalar* src_buf0, const Scalar* src_buf1, const Scalar* src_buf2, Scalar* dst_buf)"
  },
  {
    "label": "PacketReturnType()",
    "kind": "Method",
    "detail": "Function (const auto src_val0 = pload<)",
    "insertText": "PacketReturnType(src_buf0 + i)"
  },
  {
    "label": "PacketReturnType()",
    "kind": "Method",
    "detail": "Function (const auto src_val1 = pload<)",
    "insertText": "PacketReturnType(src_buf1 + i)"
  },
  {
    "label": "PacketReturnType()",
    "kind": "Method",
    "detail": "Function (const auto src_val2 = pload<)",
    "insertText": "PacketReturnType(src_buf2 + i)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (const auto dst_val = ploadt<PacketReturnType,)",
    "insertText": "Alignment(dst_buf + i)"
  },
  {
    "label": "padd()",
    "kind": "Method",
    "detail": "Function (const auto sum =)",
    "insertText": "padd(padd(dst_val, src_val0), padd(src_val1, src_val2))"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (pstoret<Scalar,PacketReturnType,)",
    "insertText": "Alignment(dst_buf + i, sum)"
  },
  {
    "label": "eval()",
    "kind": "Method",
    "detail": "Function (dst_buf [ i ] + = src_buf0 [ i ] + src_buf1 [ i ] + src_buf2 [ i ] ; } } template<int Alignment> void)",
    "insertText": "eval(Barrier& barrier, Index start_block_idx, Index end_block_idx)"
  },
  {
    "label": "mid_block_idx()",
    "kind": "Method",
    "detail": "Function (Index)",
    "insertText": "mid_block_idx(start_block_idx + end_block_idx)"
  },
  {
    "label": "enqueueNoNotification()",
    "kind": "Method",
    "detail": "Function (evaluator -> m_device .)",
    "insertText": "enqueueNoNotification([this, &barrier, mid_block_idx, end_block_idx]() { eval<Alignment>(barrier, mid_block_idx, end_block_idx); })"
  },
  {
    "label": "actualBlockSize()",
    "kind": "Method",
    "detail": "Function (end_block_idx = mid_block_idx ; } Index block_idx = start_block_idx ; Index block_start = block_idx* block_size ; Index block_end = block_start +)",
    "insertText": "actualBlockSize(block_idx)"
  },
  {
    "label": "Alignment()",
    "kind": "Method",
    "detail": "Function (processBlock<)",
    "insertText": "Alignment(block_idx, block_start, block_end)"
  },
  {
    "label": "evalAsync()",
    "kind": "Method",
    "detail": "Function (} template<int Alignment> void)",
    "insertText": "evalAsync(Index start_block_idx, Index end_block_idx)"
  },
  {
    "label": "enqueueNoNotification()",
    "kind": "Method",
    "detail": "Function (evaluator -> m_device .)",
    "insertText": "enqueueNoNotification([this, mid_block_idx, end_block_idx]() { evalAsync<Alignment>(mid_block_idx, end_block_idx); })"
  },
  {
    "label": "blockSize()",
    "kind": "Method",
    "detail": "Function (} } Index)",
    "insertText": "blockSize(Index k, int num_threads)"
  },
  {
    "label": "round_up()",
    "kind": "Method",
    "detail": "Function (Index)",
    "insertText": "round_up(Index index)"
  },
  {
    "label": "Index()",
    "kind": "Method",
    "detail": "Function (const Index kmultiple = packet_size<= 8 ? 8 : packet_size ; return divup<)",
    "insertText": "Index(index, kmultiple)"
  },
  {
    "label": "round_up()",
    "kind": "Method",
    "detail": "Function (} ; const Index target_block_size =)",
    "insertText": "round_up(divup<Index>(k, num_threads))"
  },
  {
    "label": "Index()",
    "kind": "Method",
    "detail": "Function (const Index desired_min_block_size = 1 2* packet_size ; return numext::mini<)",
    "insertText": "Index(k, numext::maxi<Index>(desired_min_block_size, target_block_size))"
  },
  {
    "label": "EvalShardedByInnerDimContext()",
    "kind": "Method",
    "detail": "Function (})",
    "insertText": "EvalShardedByInnerDimContext(const EvalShardedByInnerDimContext&)"
  },
  {
    "label": "operator()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "operator(const EvalShardedByInnerDimContext&)"
  },
  {
    "label": "shardByCol()",
    "kind": "Method",
    "detail": "Function (} ; bool)",
    "insertText": "shardByCol(Index m, Index n, Index num_threads)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (Index nm1 = nm0 ;)",
    "insertText": "for(;;)"
  },
  {
    "label": "checkGrain()",
    "kind": "Method",
    "detail": "Function (int res =)",
    "insertText": "checkGrain(m, n, bm, bn, bk, gm1, gn, gm, gn, num_threads, shard_by_col)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (nm1 =)",
    "insertText": "divup(nm0, gm1)"
  },
  {
    "label": "coarsenN()",
    "kind": "Method",
    "detail": "Function (gm = gm1 ; } return gm ; } Index)",
    "insertText": "coarsenN(Index m, Index n, Index bm, Index bn, Index bk, Index gm, int num_threads, bool shard_by_col)"
  },
  {
    "label": "checkGrain()",
    "kind": "Method",
    "detail": "Function (int res =)",
    "insertText": "checkGrain(m, n, bm, bn, bk, gm, gn1, gm, gn, num_threads, shard_by_col)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (nn1 =)",
    "insertText": "divup(nn0, gn1)"
  },
  {
    "label": "checkGrain()",
    "kind": "Method",
    "detail": "Function (gn = gn1 ; } return gn ; } int)",
    "insertText": "checkGrain(Index m, Index n, Index bm, Index bn, Index bk, Index gm, Index gn, Index oldgm, Index oldgn, int num_threads, bool shard_by_col)"
  },
  {
    "label": "contractionCost()",
    "kind": "Method",
    "detail": "Function (const TensorOpCost cost =)",
    "insertText": "contractionCost(bm * gm, bn * gn, bm, bn, bk, shard_by_col, true)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (Index new_tasks =)",
    "insertText": "divup(nm0, gm) * divup(nn0, gn)"
  },
  {
    "label": "divup()",
    "kind": "Method",
    "detail": "Function (Index old_tasks =)",
    "insertText": "divup(nm0, oldgm) * divup(nn0, oldgn)"
  },
  {
    "label": "int()",
    "kind": "Method",
    "detail": "Function (const int packed_size = std::min<)",
    "insertText": "int(PacketType<LhsScalar, Device>::size, PacketType<RhsScalar, Device>::size)"
  },
  {
    "label": "double()",
    "kind": "Method",
    "detail": "Function (const int output_packet_size = internal::unpacket_traits<PacketReturnType>::size ; const double kd = static_cast<)",
    "insertText": "double(bk)"
  },
  {
    "label": "TensorOpCost()",
    "kind": "Method",
    "detail": "Function (TensorOpCost cost =)",
    "insertText": "TensorOpCost(0, 0, kd * compute_bandwidth, true, packed_size)"
  },
  {
    "label": "TensorOpCost()",
    "kind": "Method",
    "detail": "Function (cost + =)",
    "insertText": "TensorOpCost(0, sizeof(CoeffReturnType), 0, true, output_packet_size)"
  },
  {
    "label": "costPerCoeff()",
    "kind": "Method",
    "detail": "Function (TensorOpCost rhsCost = this -> m_rightImpl .)",
    "insertText": "costPerCoeff(true) * (kd / m)"
  },
  {
    "label": "sizeof()",
    "kind": "Method",
    "detail": "Function (std::ptrdiff_t bufsize = m* n*)",
    "insertText": "sizeof(Scalar)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (bool shard_by_k = false ;)",
    "insertText": "if(n == 1 || num_threads_by_k < 2 || num_threads_by_k < num_threads || bufsize > l3CacheSize() / num_threads_by_k || k / num_threads_by_k < 2 * Traits::nr)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (shard_by_k = false ; } else)",
    "insertText": "if(numext::maxi(m, n) / num_threads < Traits::nr || (k / num_threads_by_k > 8 * Traits::nr && (numext::mini(m, n) < 2 * Traits::nr || num_threads_by_k > num_threads)))"
  },
  {
    "label": "contractionCostPerInnerDim()",
    "kind": "Method",
    "detail": "Function (shard_by_k = true ; } return shard_by_k ; } TensorOpCost)",
    "insertText": "contractionCostPerInnerDim(Index m, Index n, Index k)"
  },
  {
    "label": "cost()",
    "kind": "Method",
    "detail": "Function (const int output_packet_size = internal::unpacket_traits<PacketReturnType>::size ; TensorOpCost)",
    "insertText": "cost(0, 0, (computeBandwidth(true, m, n, k) * m) * n, true, output_packet_size)"
  },
  {
    "label": "costPerCoeff()",
    "kind": "Method",
    "detail": "Function (TensorOpCost lhsCost = this -> m_leftImpl .)",
    "insertText": "costPerCoeff(true)"
  },
  {
    "label": "dropMemoryCost()",
    "kind": "Method",
    "detail": "Function (lhsCost .)",
    "insertText": "dropMemoryCost()"
  },
  {
    "label": "contractionCostPerInnerDim()",
    "kind": "Method",
    "detail": "Function (const int output_packet_size = internal::unpacket_traits<PacketReturnType>::size ; TensorOpCost cost =)",
    "insertText": "contractionCostPerInnerDim(m, n, k)"
  },
  {
    "label": "for()",
    "kind": "Method",
    "detail": "Function (int num_threads = 1 ; double min_cost = total_parallel_cost ; double kPerThreadOverHead = 3 0 0 0 ; double kFixedOverHead = 1 0 0 0 0 0 ;)",
    "insertText": "for(int nt = 2; nt <= this->m_device.numThreads(); nt += 2)"
  },
  {
    "label": "computeBandwidth()",
    "kind": "Method",
    "detail": "Function (num_threads = nt ; min_cost = parallel_cost ; } } return num_threads ; } double)",
    "insertText": "computeBandwidth(bool shard_by_col, Index bm, Index bn, Index bk)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (# ifndef EIGEN_VECTORIZE_FMA)",
    "insertText": "if(computeBandwidth == 0.5)"
  }
]