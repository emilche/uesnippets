[
  {
    "label": "defined()",
    "kind": "Method",
    "detail": "Function (# if !)",
    "insertText": "defined(__TBB_machine_H) || defined(__TBB_machine_linux_ia32_H)"
  },
  {
    "label": "__TBB_compiler_fence()",
    "kind": "Method",
    "detail": "Function (use public TBB headers instead . # endif # define __TBB_machine_linux_ia32_H # include<stdint . h> # include \" gcc_ia32_common . h \" # define __TBB_WORDSIZE 4 # define __TBB_ENDIANNESS __TBB_ENDIAN_LITTLE # define)",
    "insertText": "__TBB_compiler_fence() __asm__ __volatile__(\"\": : :\"memory\") #define __TBB_control_consistency_helper() __TBB_compiler_fence() #define __TBB_acquire_consistency_helper() __TBB_compiler_fence() #define __TBB_release_consistency_helper() __TBB_compiler_fence() #define __TBB_full_memory_fence() __asm__ __volatile__(\"mfence\": : :\"memory\") #if __TBB_ICC_ASM_VOLATILE_BROKEN #define __TBB_VOLATILE #else #define __TBB_VOLATILE volatile #endif #define __TBB_MACHINE_DEFINE_ATOMICS(S,T,X,R) \\ static inline T __TBB_machine_cmpswp##S (volatile void *ptr, T value, T comparand)"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (\\ T result ; \\ \\ __asm__)",
    "insertText": "__volatile__(\"lock\\ncmpxchg\" X \" %2,%1\" \\ : \"=a\"(result), \"=m\"(*(__TBB_VOLATILE T*)ptr) \\ : \"q\"(value), \"0\"(comparand), \"m\"(*(__TBB_VOLATILE T*)ptr) \\ : \"memory\")"
  },
  {
    "label": "S()",
    "kind": "Method",
    "detail": "Function (\\ return result ; \\ } \\ \\ T __TBB_machine_fetchadd # #)",
    "insertText": "S(volatile void *ptr, T addend)"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (\\ T result ; \\ __asm__)",
    "insertText": "__volatile__(\"lock\\nxadd\" X \" %0,%1\" \\ : R (result), \"=m\"(*(__TBB_VOLATILE T*)ptr) \\ : \"0\"(addend), \"m\"(*(__TBB_VOLATILE T*)ptr) \\ : \"memory\")"
  },
  {
    "label": "S()",
    "kind": "Method",
    "detail": "Function (\\ return result ; \\ } \\ \\ T __TBB_machine_fetchstore # #)",
    "insertText": "S(volatile void *ptr, T value)"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (\\ T result ; \\ __asm__)",
    "insertText": "__volatile__(\"lock\\nxchg\" X \" %0,%1\" \\ : R (result), \"=m\"(*(__TBB_VOLATILE T*)ptr) \\ : \"0\"(value), \"m\"(*(__TBB_VOLATILE T*)ptr) \\ : \"memory\")"
  },
  {
    "label": "__TBB_MACHINE_DEFINE_ATOMICS()",
    "kind": "Method",
    "detail": "Function (\\ return result ; \\ } \\)",
    "insertText": "__TBB_MACHINE_DEFINE_ATOMICS(1,int8_t,\"\",\"=q\") __TBB_MACHINE_DEFINE_ATOMICS(2,int16_t,\"\",\"=r\") __TBB_MACHINE_DEFINE_ATOMICS(4,int32_t,\"l\",\"=r\") #if __INTEL_COMPILER #pragma warning( push ) #pragma warning( disable: 998 ) #endif #if __TBB_GCC_CAS8_BUILTIN_INLINING_BROKEN #define __TBB_IA32_CAS8_NOINLINE __attribute__ ((noinline)) #else #define __TBB_IA32_CAS8_NOINLINE #endif static inline __TBB_IA32_CAS8_NOINLINE int64_t __TBB_machine_cmpswp8 (volatile void *ptr, int64_t value, int64_t comparand)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (#)",
    "insertText": "if(__TBB_GCC_BUILTIN_ATOMICS_PRESENT || (__TBB_GCC_VERSION >= 40102)) && !__TBB_GCC_64BIT_ATOMIC_BUILTINS_BROKEN return __sync_val_compare_and_swap( reinterpret_cast<volatile int64_t*>(ptr), comparand, value)"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (# else int64_t result ; union { int64_t i64 ; int32_t i32 [ 2 ] ; } ; i64 = value ; # if __PIC__ int32_t tmp ; __asm__)",
    "insertText": "__volatile__(\"movl %%ebx,%2\\n\\t\" \"movl %5,%%ebx\\n\\t\" #if __GNUC__==3 \"lock\\n\\t cmpxchg8b %1\\n\\t\" #else \"lock\\n\\t cmpxchg8b (%3)\\n\\t\" #endif \"movl %2,%%ebx\" : \"=A\"(result) , \"=m\"(*(__TBB_VOLATILE int64_t *)ptr) , \"=m\"(tmp) #if __GNUC__==3 : \"m\"(*(__TBB_VOLATILE int64_t *)ptr) #else : \"SD\"(ptr) #endif , \"0\"(comparand) , \"m\"(i32[0]), \"c\"(i32[1]) : \"memory\" #if __INTEL_COMPILER ,\"ebx\" #endif)"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (# else __asm__)",
    "insertText": "__volatile__(\"lock\\n\\t cmpxchg8b %1\\n\\t\" : \"=A\"(result), \"=m\"(*(__TBB_VOLATILE int64_t *)ptr) : \"m\"(*(__TBB_VOLATILE int64_t *)ptr) , \"0\"(comparand) , \"b\"(i32[0]), \"c\"(i32[1]) : \"memory\")"
  },
  {
    "label": "warning()",
    "kind": "Method",
    "detail": "Function (# endif return result ; # endif } # undef __TBB_IA32_CAS8_NOINLINE # if __INTEL_COMPILER # pragma)",
    "insertText": "warning(pop ) #endif static inline void __TBB_machine_or( volatile void *ptr, uint32_t addend)"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (__asm__)",
    "insertText": "__volatile__(\"lock\\norl %1,%0\" : \"=m\"(*(__TBB_VOLATILE uint32_t *)ptr) : \"r\"(addend), \"m\"(*(__TBB_VOLATILE uint32_t *)ptr) : \"memory\")"
  },
  {
    "label": "__TBB_machine_and()",
    "kind": "Method",
    "detail": "Function (} void)",
    "insertText": "__TBB_machine_and(volatile void *ptr, uint32_t addend)"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (__asm__)",
    "insertText": "__volatile__(\"lock\\nandl %1,%0\" : \"=m\"(*(__TBB_VOLATILE uint32_t *)ptr) : \"r\"(addend), \"m\"(*(__TBB_VOLATILE uint32_t *)ptr) : \"memory\")"
  },
  {
    "label": "__TBB_machine_aligned_load8()",
    "kind": "Method",
    "detail": "Function (} # if __clang__ # define __TBB_fildq \" fildll \" # define __TBB_fistpq \" fistpll \" # else # define __TBB_fildq \" fildq \" # define __TBB_fistpq \" fistpq \" # endif int64_t)",
    "insertText": "__TBB_machine_aligned_load8(const volatile void *ptr)"
  },
  {
    "label": "__TBB_ASSERT()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "__TBB_ASSERT(tbb::internal::is_aligned(ptr,8),\"__TBB_machine_aligned_load8 should be used with 8 byte aligned locations only \\n\")"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (int64_t result ; __asm__)",
    "insertText": "__volatile__(__TBB_fildq \" %1\\n\\t\" __TBB_fistpq \" %0\" : \"=m\"(result) : \"m\"(*(const __TBB_VOLATILE uint64_t*)ptr) : \"memory\")"
  },
  {
    "label": "__TBB_ASSERT()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "__TBB_ASSERT(tbb::internal::is_aligned(ptr,8),\"__TBB_machine_aligned_store8 should be used with 8 byte aligned locations only \\n\")"
  },
  {
    "label": "__volatile__()",
    "kind": "Method",
    "detail": "Function (__asm__)",
    "insertText": "__volatile__(__TBB_fildq \" %1\\n\\t\" __TBB_fistpq \" %0\" : \"=m\"(*(__TBB_VOLATILE int64_t*)ptr) : \"m\"(value) : \"memory\")"
  },
  {
    "label": "__TBB_machine_load8()",
    "kind": "Method",
    "detail": "Function (} int64_t)",
    "insertText": "__TBB_machine_load8(const volatile void *ptr)"
  },
  {
    "label": "if()",
    "kind": "Method",
    "detail": "Function (# if __TBB_FORCE_64BIT_ALIGNMENT_BROKEN)",
    "insertText": "if(tbb::internal::is_aligned(ptr,8))"
  },
  {
    "label": "__TBB_machine_aligned_load8()",
    "kind": "Method",
    "detail": "Function (# endif return)",
    "insertText": "__TBB_machine_aligned_load8(ptr)"
  },
  {
    "label": "__TBB_machine_cmpswp8()",
    "kind": "Method",
    "detail": "Function (# if __TBB_FORCE_64BIT_ALIGNMENT_BROKEN } else { return)",
    "insertText": "__TBB_machine_cmpswp8(const_cast<void*>(ptr),0,0)"
  },
  {
    "label": "__TBB_machine_store8_slow()",
    "kind": "Method",
    "detail": "Function (} # endif } \" C \" void)",
    "insertText": "__TBB_machine_store8_slow(volatile void *ptr, int64_t value)"
  },
  {
    "label": "__TBB_machine_store8_slow_perf_warning()",
    "kind": "Method",
    "detail": "Function (\" C \" void)",
    "insertText": "__TBB_machine_store8_slow_perf_warning(volatile void *ptr)"
  },
  {
    "label": "__TBB_machine_store8()",
    "kind": "Method",
    "detail": "Function (void)",
    "insertText": "__TBB_machine_store8(volatile void *ptr, int64_t value)"
  },
  {
    "label": "__TBB_machine_aligned_store8()",
    "kind": "Method",
    "detail": "Function (# endif)",
    "insertText": "__TBB_machine_aligned_store8(ptr,value)"
  },
  {
    "label": "__TBB_machine_store8_slow_perf_warning()",
    "kind": "Method",
    "detail": "Function (# if __TBB_FORCE_64BIT_ALIGNMENT_BROKEN } else { # if TBB_USE_PERFORMANCE_WARNINGS)",
    "insertText": "__TBB_machine_store8_slow_perf_warning(ptr)"
  },
  {
    "label": "__TBB_machine_store8_slow()",
    "kind": "Method",
    "detail": "Function (# endif)",
    "insertText": "__TBB_machine_store8_slow(ptr,value)"
  }
]